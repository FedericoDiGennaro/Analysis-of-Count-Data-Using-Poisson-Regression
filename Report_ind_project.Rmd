---
title: " A Statistical Analysis of count data"
subtitle: "MATH-493 - Applied Biostatistics: individual project"

#date: "`r Sys.Date()`"

author: " Federico Di Gennaro"


geometry: margin=2.5cm
fontsize: 12pt
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
#- \fancyhead[CO,CE]{Applied Biostatistics Report - Individual project}
- \fancyfoot[CO,CE]{Federico Di Gennaro}
- \fancyfoot[LE,RO]{\thepage}

output: pdf_document

---

# Introduction 
This statistical analysis examines the relationship between the apprentice migration between 1775 and 1799 to Edinburgh from 33 regions in Scotland.
The study utilizes the following four variables to try to predict the number of apprentices migration:

* Distance: Distance (in km ???) from the region to Edinburgh. 
* Population: Population (1000s) in the region. 
* Degree_Urb: Degree of urbanization of the region (in %). 
* Direction: Categorical variables that takes value in $\{1,2,3\}$ stays for: 1=North, 2=West, 3=South. 

The outcome variable, as said before, is the number apprentices migration that in this analysis we called "Apprentices". 
It is immediately important to notice the variables we are working with; in particular, we can notice that the outcome variable "Apprentices" can be seen as "count data", taking only discrete values. For that reason, we will not end up with an outcome normally distributed and so linear regression cannot be used.

From literature otherwise, we know that Poisson regression is suitable to analyse count data (in this case our $Y$ is a the number of apprentices, i.e. it can be seen as count data).
Poisson regression is in the family of the so-called Generalized Linear Models (GLM).  
Generalized Linear Models are models in which response variables follow a distribution other than the normal distribution. In GLMs, the response variable is connected to the linear predictor $\eta =  \beta_0 + \beta_1X_1 + ... + \beta_k X_k$ by a link function $g(\cdot)$, that describe the functional relationship between $\eta$ and the mathematical expectation of the response variable: $g(\mathbb{E}[Y|x])=\eta$. For Poisson Regression, $g(x)=log(x)$. 

Poisson regression relies on Poisson distribution; we say that a discrete random variable X is distributed as a Poisson with parameter $\lambda$ ($X \sim Poisson(\lambda)$) if it has the following density function: $$\mathbb{P}(X=k)=\frac{\lambda^ke^{-\lambda}}{k!}$$

# Exploratory Data Analysis
Before starting in fitting the model, it is a good practice to explore the available data. In this way, we can already notice some insights about what the model will tell us and in which way each variable can effect the model itself.
The dataset provided has 33 rows, corresponding to the different counties of Scotland under study, and for each row we have the dependent variable under study (Apprentices), and the registered values for the regressors mentioned in the introduction. 

```{r setup, include = FALSE}
# Setup options for R Markdown
library(ggplot2)
library(tidyverse)
library(GGally)
library(readxl)
library(MASS) # for BOXCOX
library(car) # for VIF
library(performance) # for nicer multicollinearity plot
library(xtable)
data <- read_excel("data/data.xlsx")

knitr::opts_chunk$set(
  echo       = FALSE,    # Do not print code
  warning    = FALSE,    # Suppress warnings
  message    = FALSE,    # Suppress messages
  fig.align  = "center", # Center figures
  fig.width  = 2.7,      # Good standard figure width for single-panel figures
  fig.height = 2.4,       # Good standard figure height for single-panel figures
  fig.pos = "ht",
  out.extra = ""
)
```

```{r plot1, fig.cap = "Pairs Plot of the explanatory variables. Their distribution is divided with respect to the value of the variable Direction (factor)", fig.width= 15,fig.height=12}
# define regressors
data$Direction <- as.factor(data$Direction)
X <- data[-c(1,3)]

# in order to check the differences in distribution due to difference in the
# union_shop variable, use this: (DISAGGREGATE DATA wrt Direction)
plot <- ggpairs(X, aes(color = Direction, alpha = .5),
                lower = list(continuous = 'smooth'),
                upper = list(continuous = wrap("cor",size = 12)), 
                legend = 1, textsize=25) +
  theme(legend.position = "bottom", text = element_text(size = 25), 
        axis.text.x = element_text(size = 20), axis.text.y = element_text(size = 20))
# Remove alpha from the legend
plot <- plot +
  scale_alpha_identity(guide = "none")
# Display the plot
print(plot)
```
From this plot, we can notice that the three variables $\textit{Distance}$, $\textit{Population}$ and $\textit{Degree Urb}$ change distribution with respect to the value of the value of the factor variable $\textit{Direction}$. Please notice also that the variables $\textit{Distance}$ and $\textit{Population}$ are really skewed; for this reason it can be worth at some point of the analysis trying to apply a logarithmic transformation to them in fitting our models.

```{r plot2, fig.cap = "Histogram of outcome variable Apprentices", fig.width= 10,fig.height=6}
# define dependent
y <- data[3]
hist(y$Apprentices, breaks=seq(0,250,10), main="Histogram of the outcome variable", xlab = "Number of Apprentices", ylab = "Frequency")
```
With the histogram of the outcome variable $\textit{Apprentices}$, I can observe the shape of the empirical distribution of that variable. Specifically, it is already noticeable that there is an observation that is really far away from the others; this can leads problems such as over-dispersion.

# Model Fitting
## Model highlights:
Let $$ Y = Apprentices; \;
X_1 = Distance; \;
X_2 = Population; \;
X_3 = Degree\_Urb; \;
X_4 = Direction$$
As already said in the introduction, we will use Poisson Regression to determine whether there is or not effect between explanatory variables $X_i, i \in \{1,2,3,4\}$ and the outcome variable $Y$.

**Model 1.**  
Let's now define the first model in the study. As already said before, let $$\eta =  \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4 X_4$$ Hence, the model is defined as $$log(\mathbb{E}[Y|x])=\eta$$

```{r}
poisson.model<-glm(Apprentices ~ Distance + Population + Degree_Urb + Direction, data,
                   family=poisson(link = "log"))
# Example table data
d <- summary(poisson.model)$coefficients
knitr::kable(d, digits = 2)
```
This problems seems to have a lot of problem.
Firstly, as already guessed in the exploratory data analysis, the model suffers of over-dispersion. The residual deviance $(256.31)$ is bigger than the degrees of freedom $(28)$. For this reason it seems that there is a larger variance in observed counts than expected from the Poisson assumption, meaning that there is over-dispersion and that the Poisson model does not fit well.  
Another way to observe the over-dispersion, is fitting a quasi-Poisson model: the output in R will have the same coefficient as $\textit{Model 1}$ but from this new model we can extrapolate the real dispersion parameter $(\phi)$ of the data that turns out to be $7731.329$ (i.e. there is a huge over-dispersion because that parameter should be around 1 in a Poisson Regression model). This problem can be related to the extremely un-related observation of variable $\textit{Apprentices}$ that we observed in the EDA. For this reason I am removing this observation from my data and then try to fit again the same model.

**Model 2.**
As already said above, this model is the same model as $\textit{Model 1}$ where I simply removed the outlier observation that probably made $\textit{Model 1}$ over-dispersive.  
Even removing the strange observation, the model still suffers of over-dispersion ($\mu=7.59$ and $\sigma^2=131.86$). In this case otherwise, the dispersion parameter is no longer as large as before $(\phi=18.43)$ and for this reason it is worth trying to work with this data.  
Due to over-dispersion, modeling this data using the Poisson family assumption yield erroneous p-values regarding model variables. For this reason I decide to use a quasi-Poisson model, that relax the assumption of $\phi=1$. In this new model, the additional over-dispersion parameter $\phi=18.43$ might capture a large amount of the variation of the response variable. Note that since the underlying data is the same, the residuals deviance is the same.  
```{r}
data <- read_excel("data/data2.xlsx")
poisson.model<-glm(Apprentices ~ Distance + Population + Degree_Urb
                    + Direction , data, family = quasipoisson())
knitr::kable(summary(poisson.model)$coefficients, digits = 2)
```
Under these assumptions we can see that the parameters significant at a level $\alpha=0.05$ are $\beta_0$(intercept), $\beta_1$ (coefficient of the variable $\textit{Distance}$) and $\beta_2$ (coefficient of the variable $\textit{Population}$).  
The significant variable $\textit{Distance}$ as a negative sign in its parameter, meaning that at an increase of $\textit{Distance}$ corresponds a decrease of the number of $\textit{Apprentices}$. The significant variable $\textit{Population}$ as a positive sign in its parameter, meaning that at an increase of $\textit{Distance}$ corresponds an increase of the number of $\textit{Apprentices}$.
From now on, I will consider this model (Quasi-Poisson regression model) as **base model**.

**Model 3.**
As said in the previous exploratory data analysis, it is worth trying to transform the explanatory variables $\textit{Distance}$ and $\textit{Population}$ with a logarithmic transformation. This can avoid problems coming from the skewednees these variables.
```{r}
data$Distance = log(data$Distance)
data$Population = log(data$Population)
# Change the names of the variables
names(data)[names(data) == "Distance"] <- "LogDistance"
names(data)[names(data) == "Population"] <- "LogPopulation"
poisson.model<-glm(Apprentices ~ LogDistance + LogPopulation + Degree_Urb
                   + Direction, data , family = poisson(link = "log"),
                   control = glm.control(maxit = 1000))

poisson.model<-glm(Apprentices ~ LogDistance + LogPopulation + Degree_Urb
                   + Direction, data , family = quasipoisson(),
                   control = glm.control(maxit = 1000))

knitr::kable(summary(poisson.model)$coefficients, digits = 2)
```
From these model, we can observe that the variables $\textit{Distance}$ and $\textit{Population}$ are even more significant then in $\textit{Model 3}$, and their interpretation does not change because the sign of their parameters is still the ones as in $\textit{Model 3}$. This problem is better than $\textit{Model 3}$ MAKE THE STATISTICAL TEST!!!!!!!
It is also good to notice that the over-dispersion parameter $\phi$ is decreasing too (now it is equal to $3.37$)

**Model 4.**
Now I am adding at the Quasi-Poisson model $\textit{Model 4}$ the interactions between the whole variables.
```{r}
poisson.model4<-glm(Apprentices ~ (LogDistance + LogPopulation + Degree_Urb
                    + Direction)^2 , data, family = quasipoisson())
knitr::kable(summary(poisson.model4)$coefficients, digits = 2)
```
From the table above, we can observe that there are several variables that are significant at level $\alpha=0.05$ but a lot of them are not. For this model, I can apply a step-wise backward elimination procedure to determine the best subset of independent variables that explain the variation in the dependent variable.

