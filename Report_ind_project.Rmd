---
title: " A Statistical Analysis ?????????"
subtitle: "MATH-493 - Applied Biostatistics: individual project"

date: "`r Sys.Date()`"

author: " Federico Di Gennaro"


geometry: margin=2.5cm
fontsize: 12pt
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{Applied Biostatistics Report - Individual project}
- \fancyfoot[CO,CE]{Federico Di Gennaro}
- \fancyfoot[LE,RO]{\thepage}

output: pdf_document

---

# Introduction 
This statistical analysis examines the relationship between the apprentice migration between 1775 and 1799 to Edinburgh from 33 regions in Scotland.

ADD SOMETHING

The study utilizes the following four variables to try to predict the number of apprentices migration:

* Distance: Distance (in km ???) from the region to Edinburgh. 
* Population: Population (1000s) in the region. 
* Degree_Urb: Degree of urbanisation of the region (in %). 
* Direction: Categorical variables that takes value in $\{1,2,3\}$ stays for: 1=North, 2=West, 3=South. 

The output variable, as said before, is the number apprentices migration that in this analysis we called "Apprentices".

ADD why we use poisson regression

# Exploratory Data Analysis
Before starting in fitting the model, it is a good practice to explore the available data. In this way, we can already notice some insights about what the model will tell us and in which way each variable can effect the model itself.
The dataset (see appendix, Table 1) has 33 rows, corresponding to the different counties of Scotland under study, and for each row we have the dependent variable under study (Apprentices), and the registered values for the regressors mentioned in the introduction. 

```{r setup, include = FALSE}
# Setup options for R Markdown

library(ggplot2)
library(tidyverse)
library(GGally)
library(readxl)
library(MASS) # for BOXCOX
library(car) # for VIF
library(performance) # for nicer multicollinearity plot
data <- read_excel("data/data.xlsx")

knitr::opts_chunk$set(
  echo       = FALSE,    # Do not print code
  warning    = FALSE,    # Suppress warnings
  message    = FALSE,    # Suppress messages
  fig.align  = "center", # Center figures
  fig.width  = 2.7,      # Good standard figure width for single-panel figures
  fig.height = 2.4,       # Good standard figure height for single-panel figures
  fig.pos = "ht",
  out.extra = ""
)


```

```{r plot1, fig.cap = "Pairs Plot", fig.width= 15,fig.height=8}
# define regressors
X <- data[-c(1,3)]
X$Direction <- as.factor(X$Direction)

# define dependent
y <- data[3]

# in order to check the differences in distribution due to difference in the
# union_shop variable, use this: (DISAGGREGATE DATA wrt Union_shop)
ggpairs(X,
        aes(color = X$Direction, alpha = .5),
        lower = list(continuous = 'smooth'), legend = 1) +
  theme(legend.position = "bottom", text = element_text(size = 14), 
        axis.text.x = element_text(size = 10), axis.text.y = element_text(size = 10) )
```

In this multivariate graphical exploration of data we kept only the continuous variables.
Then we plot their distribution divided by the value of the binary variable Union_Shop (on the diagonal). Then outside the diagonal we can find the correlation plot within each possible pairs. The division with respect to the binary variable, helps us to disaggregate data to understand them better and avoid mis-interpretation (Simpson paradox).
In particular in this example we can find out that p-workforce is the variable most affected by the value of the binary variable.

```{r plot2, fig.cap = "Distribution of different variables", fig.width= 15,fig.height=8}
par(mfrow=c(2,2))

qqnorm(data$p_stop, pch = 1, frame = FALSE, main = "p_stop")
qqline(data$p_stop, col = "steelblue", lwd = 2)

qqnorm(data$p_workforce, pch = 1, frame = FALSE, main = "p_workforce")
qqline(data$p_workforce, col = "steelblue", lwd = 2)

qqnorm(data$p_SectorA, pch = 1, frame = FALSE, main = "p_SectorA")
qqline(data$p_SectorA, col = "steelblue", lwd = 2)

qqnorm(data$p_agriculture, pch = 1, frame = FALSE, main = "p_agriculture")
qqline(data$p_agriculture, col = "steelblue", lwd = 2)

par(cex.lab = 2)
```

# Model Fitting
## Model highlights:
**Base model.**
We firstly started by a simple regression model using all the variables and with no interactions. We consider this as our "base model" that we start to investigate in order to gain insights about what will happen in model selection.
Let 
$$
Y = p\_stop; \;
X_1 = p\_workforce; \;
X_2 = Union\_shop ; \;
X_3 = p\_SectorA; \;
X_4 = p\_agriculture.
$$
Then, we fit the following model: 
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \epsilon
\end{equation}
We fit this model with R using `lm()` command and we obtained
```{r}
model <- lm(p_stop ~ p_workforce + Union_shop + p_SectorA + p_agriculture, data=data)
knitr::kable(summary(model)$coefficients)
#summary(model)
```
The output of this simple model already shows us some good insights. Firstly, there is only one variable whose coefficient is significantly different from zero (t-test $H_0: \beta_i = 0$). Then, the adjusted $R^2$ obtained is 0.2733, proving that the model seems to perform quite poorly in this regression task. Even performing a forward or backward selection does not help us to increase the adjusted $R^2$ because as already suggested by the output above the only variable kept by these step selection methods was the only one that were significant in the base model ("p_workforce").
For this reason, we started to investigate other kind of relationship between $Y$ and the explanatory variables $X_i$.

**Best model.**
Our first guess in order to increase the variance explained by our model is to include all possible two-way interactions between the independent variables. As we already highlighted in the exploratory data analysis in fact, there is some level of interaction between the regressors. Therefore, it is good habit to include the interaction terms inside the model.

We have also decided to apply Box-Cox transformation to our data. At the core of the Box-Cox transformation is an exponent, lambda ($\lambda$). We pick the $\lambda$ that maximizes the log-likelihood of the data. The “optimal value” is the one which results in the best approximation of a normal distribution curve. This value is then used to transform the dependent variable "p_stop".
Let's seek for the best value of lambda in [Figure 3].
```{r plot3, fig.cap = "Box Cox: Best lambda", fig.width=8,fig.height=4}
model_box <- lm(p_stop ~ (p_workforce + Union_shop + p_SectorA + p_agriculture)^2, data=data)
bc <- boxcox(model_box, interp = TRUE)
lambda <- bc$x[which.max(bc$y)]
abline(v = lambda, col = "red")
points(x=lambda,y=-485, lwd=2, pch =19, col ="red")
text(x=.45,y=-185, col="black", labels=paste('0.303'), pos=1)
```
According to this plot, the best value for $\lambda$ is around $\frac{1}{3}$. Can be used to apply the following transformation to our outcome variable:
$$\frac{y^{\lambda}-1}{\lambda}$$

Finally, the regression mode, where $\frac{y^{\lambda}-1}{\lambda}$ is modelled as:
\begin{equation}
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 +  \beta_5 X_1 \cdot X_2 + \beta_{6} X_1 \cdot X_3 + \beta_{7} X_1 \cdot X_4 + \beta_{8} X_2 \cdot X_3 + \beta_{9} X_2 \cdot X_4 + \beta_{10} X_3 \cdot X_4 + \epsilon
\end{equation}
```{r}
data_bc <- data
data_bc$p_stop <- (data_bc$p_stop^lambda -1)/lambda
model_bc <- lm(p_stop ~ (p_workforce + Union_shop + p_SectorA + p_agriculture)^2, data=data_bc)

final_model_bc <- step(model_bc, direction = "backward", trace = FALSE)
```
Finally, we perform a linear regression (using `lm()` command in R) on this model and we obtain the following output:
```{r}
data_bc <- data
data_bc$p_stop <- (data_bc$p_stop^lambda -1)/lambda
model_bc <- lm(p_stop ~ (p_workforce + Union_shop + p_SectorA + p_agriculture)^2, 
               data=data_bc)
knitr::kable(summary(model_bc)$coefficients)
#summary(model_bc)
```
Again a step wise backward elimination procedure is used to determine the best subset of independent variables that explain the variation in the dependent variable. 
Once the model has been fitted and backward elimination has been performed, the final (and best) model becomes the following:
\begin{equation}
\hat{y} = -0.31 + 0.06 p\_workforce + -0.05 p\_SectorA + 0.06(Union\_shop \cdot p\_SectorA)
\end{equation}
where all the coefficients are strongly significant (p-value < 0.01). 


# Model Evaluation 
Before rushing into the conclusions, it's important to verify whether the underlying assumptions of the linear regression model are satisfied, and whether the model is suffering from multicollinearity.

## Linear regression assumptions
The linear regression model has a number of underlying assumptions that need to be met in order for the model to be valid and the estimated coefficients to be reliable. These assumptions are:

* Linearity: The relationship between the dependent variable and each independent variable is linear.
* Independence: The observations in the dataset are independent of each other.
* Homoscedasticity: The variance of the error term is constant across all levels of the independent variable(s).
* Normality: The error term is normally distributed with mean zero and variance $\sigma^2$.
* No multicollinearity: There is no perfect or high correlation among the independent variables.
* No auto-correlation: The error terms are not correlated with each other.

If any of these assumptions are violated, it can lead to biased or inefficient estimates of the regression coefficients and incorrect inferences. In order to check on them, we can look at Figure 4 and Figure 5 below. 
```{r plot4, fig.cap = "Checking model assumptions", fig.width= 8,fig.height=5}
par(mfrow = c(2,2))
plot(final_model_bc)
#shapiro.test(resid(model))
```
On the top left corner of Figure 4, we have the the Residual plot: residuals against the predicted values, the plot does not show any clear patterns, and the residuals are correctly scattered around zero, verifying the assumption of linearity. 
To check on the normality, is the top right corner of  Figure 4 that we should focus on: in this plot, the x-axis represents the quantiles of the theoretical distribution (in this case, the normal distribution), while the y-axis represents the quantiles of the dataset being analyzed, and since the points are following a straight line, the dataset is normally distributed. 
If this is not enough, Shapiro-Wilk normality test is also rejecting the null of 'non-normality'. As for the homoscedasticity (equal variance of residuals across the range of predicted values): we can check it using the plot on the bottom left of Figure 4 or a statistical test such as the Breusch-Pagan test. Since the points don't show any specific pattern and seems to be randomly scattered around the horizontal line, the assumption of homoscedasticity is also satisfied. 
```{r plot5, fig.cap = "Multicollinearity check", fig.width= 6,fig.height=4}
model <- lm(p_stop ~ (p_workforce + Union_shop + p_SectorA + p_agriculture), data=data_bc)
collinearity <- check_collinearity(model)
plot(collinearity)
```
For multicollinearity, a glimpse at Figure 5 (which gives the VIF estimate for the 4 different regressors) immediately tells that the assumption is very well verified. Please, note that the interactions term have not been included, simply because high correlation among interaction terms and main effects is normal, and expected. 

Overall, we can safely state that the assumption are successfully verified.

# Conclusions
This study found that the presence or absence of right-to-work laws does not significantly affect levels of industrial conflict, except if combined with the the size of the agricultural labor force. This comes from the fact that in states with right-to-work laws, the agricultural sector is more important than the manufacturing sector, while in union-shop states, the manufacturing sector is more important than agriculture. When the agricultural sector is relatively non-unionized and significant in a state's economy, it has a major impact on the level of industrial conflict, as predicted by theory. 
The degree of unionization was identified as the major factor influencing the variation in industrial conflict levels. The conclusion is that right-to-work laws have not limited union strike power, and if section 14b of the Taft-Hartley Act is repealed, there should not be an increase in industrial conflict.





