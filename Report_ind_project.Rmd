---
title: " A Statistical Analysis of count data"
subtitle: "MATH-493 - Applied Biostatistics: individual project"

#date: "`r Sys.Date()`"

author: " Federico Di Gennaro"


geometry: margin=2.5cm
fontsize: 12pt
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
#- \fancyhead[CO,CE]{Applied Biostatistics Report - Individual project}
- \fancyfoot[CO,CE]{Federico Di Gennaro}
- \fancyfoot[LE,RO]{\thepage}

output: pdf_document

---

# Introduction 
This statistical analysis examines the relationship between the apprentice migration between 1775 and 1799 to Edinburgh from 33 regions in Scotland.
The study utilizes the following four variables to try to predict the number of apprentices migration:

* Distance: Distance (in km ???) from the region to Edinburgh. 
* Population: Population (1000s) in the region. 
* Degree_Urb: Degree of urbanization of the region (in %). 
* Direction: Categorical variables that takes value in $\{1,2,3\}$ stays for: 1=North, 2=West, 3=South. 

The outcome variable, as said before, is the number apprentices migration that in this analysis we called "Apprentices". 
It is immediately important to notice the variables we are working with; in particular, we can notice that the outcome variable "Apprentices" can be seen as "count data", taking only discrete values. For that reason, we will not end up with an outcome normally distributed and so linear regression cannot be used.

From literature otherwise, we know that Poisson regression is suitable to analyse count data (in this case our $Y$ is a the number of apprentices, i.e. it can be seen as count data).
Poisson regression is in the family of the so-called Generalized Linear Models (GLM).  
Generalized Linear Models are models in which response variables follow a distribution other than the normal distribution. In GLMs, the response variable is connected to the linear predictor $\eta =  \beta_0 + \beta_1X_1 + ... + \beta_k X_k$ by a link function $g(\cdot)$, that describe the functional relationship between $\eta$ and the mathematical expectation of the response variable: $g(\mathbb{E}[Y|x])=\eta$. For Poisson Regression, $g(x)=log(x)$. 

Poisson regression relies on Poisson distribution; we say that a discrete random variable X is distributed as a Poisson with parameter $\lambda$ ($X \sim Poisson(\lambda)$) if it has the following density function: $$\mathbb{P}(X=k)=\frac{\lambda^ke^{-\lambda}}{k!}$$

# Exploratory Data Analysis
Before starting in fitting the model, it is a good practice to explore the available data. In this way, we can already notice some insights about what the model will tell us and in which way each variable can effect the model itself.
The dataset provided has 33 rows, corresponding to the different counties of Scotland under study, and for each row we have the dependent variable under study (Apprentices), and the registered values for the regressors mentioned in the introduction. 

```{r setup, include = FALSE}
# Setup options for R Markdown
library(ggplot2)
library(tidyverse)
library(GGally)
library(readxl)
library(MASS) # for BOXCOX
library(car) # for VIF
library(performance) # for nicer multicollinearity plot
library(xtable)
data <- read_excel("data/data.xlsx")

knitr::opts_chunk$set(
  echo       = FALSE,    # Do not print code
  warning    = FALSE,    # Suppress warnings
  message    = FALSE,    # Suppress messages
  fig.align  = "center", # Center figures
  fig.width  = 2.7,      # Good standard figure width for single-panel figures
  fig.height = 2.4,       # Good standard figure height for single-panel figures
  fig.pos = "ht",
  out.extra = ""
)
```

```{r plot1, fig.cap = "Pairs Plot of the explanatory variables. Their distribution is divided with respect to the value of the variable Direction (factor)", fig.width= 15,fig.height=12}
# define regressors
data$Direction <- as.factor(data$Direction)
X <- data[-c(1,3)]

# in order to check the differences in distribution due to difference in the
# union_shop variable, use this: (DISAGGREGATE DATA wrt Direction)
plot <- ggpairs(X, aes(color = Direction, alpha = .5),
                lower = list(continuous = 'smooth'),
                upper = list(continuous = wrap("cor",size = 12)), 
                legend = 1, textsize=25) +
  theme(legend.position = "bottom", text = element_text(size = 25), 
        axis.text.x = element_text(size = 20), axis.text.y = element_text(size = 20))
# Remove alpha from the legend
plot <- plot +
  scale_alpha_identity(guide = "none")
# Display the plot
print(plot)
```
From this plot, we can notice that the three variables $\textit{Distance}$, $\textit{Population}$ and $\textit{Degree Urb}$ change distribution with respect to the value of the value of the factor variable $\textit{Direction}$. Please notice also that the variables $\textit{Distance}$ and $\textit{Population}$ are really skewed; for this reason it can be worth at some point of the analysis trying to apply a logarithmic transformation to them in fitting our models.

```{r plot2, fig.cap = "Histogram of outcome variable Apprentices", fig.width= 10,fig.height=6}
# Generate some example data
set.seed(0)
y <- data[3]

# Plot histogram of observed y and Poisson distribution
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))

# Plot histogram of observed y
hist(y$Apprentices, breaks=seq(0,250,10), col = "skyblue", border = "black", main = "Histogram of Observed y",
     xlab = "Value", ylab = "Frequency")

# Plot histogram of Poisson distribution
lambda <- mean(y$Apprentices)
x <- 0:max(y$Apprentices)
poisson_probs <- dpois(x, lambda)
barplot(poisson_probs, names.arg = x, col = "lightgreen", border = "black",
        main = "Histogram of Poisson Distribution", xlab = "Value", ylab = "Probability")

# define dependent

#hist(y$Apprentices, breaks=seq(0,250,10), main="Histogram of the outcome variable", #xlab = "Number of Apprentices", ylab = "Frequency")

```
With the histogram of the outcome variable $\textit{Apprentices}$, I can observe the shape of the empirical distribution of that variable. Specifically, it is already noticeable that there is an observation that is really far away from the others; this can leads problems such as over-dispersion.

# Model Fitting
## Model highlights:
Let $$ Y = Apprentices; \;
X_1 = Distance; \;
X_2 = Population; \;
X_3 = Degree\_Urb; \;
X_4 = Direction$$
As already said in the introduction, we will use Poisson Regression to determine whether there is or not effect between explanatory variables $X_i, i \in \{1,2,3,4\}$ and the outcome variable $Y$.

**Model 1.**  
Let's now define the first model in the study. As already said before, let $$\eta =  \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4 X_4$$ Hence, the model is defined as $$log(\mathbb{E}[Y|x])=\eta$$

```{r}
poisson.model<-glm(Apprentices ~ Distance + Population + Degree_Urb + Direction, data,
                   family=poisson(link = "log"))
# Example table data
d <- summary(poisson.model)$coefficients
knitr::kable(d, digits = 2)
```
This model seems to have a lot of problems.
Firstly, as already guessed in the exploratory data analysis, the model suffers of over-dispersion. The residual deviance $(256.31)$ is bigger than the degrees of freedom $(28)$. For this reason it seems that there is a larger variance in observed counts than expected from the Poisson assumption, meaning that there is over-dispersion and that the Poisson model does not fit well.  
Another way to observe the over-dispersion, is fitting a quasi-Poisson model: the output in R will have the same coefficient as $\textit{Model 1}$ but from this new model we can extrapolate the real dispersion parameter $(\phi)$ of the data that turns out to be $7731.329$ (i.e. there is an incredibly huge over-dispersion because that parameter should be around 1 in a Poisson Regression model). Please observe that we should not be fooled by the super significant coefficients. Those extremely low p-values are exactly the consequences of the over-dispersion issue. Without adjusting for over-dispersion in fact, we use incorrect and artificially small standard errors leading to artificially small p-values for model coefficients.  

**Model 2.**
The problem of over-dispersion, can be caused by the extremely outlier observation of variable $\textit{Apprentices}$ that we already observed in the EDA. For this reason I am removing this observation from my data and then try to fit again the same model stated for $\textit{Model 1}$.  
Even removing the strange observation, the model still suffers of over-dispersion ($\mu=7.59$ and $\sigma^2=131.86$). In this case otherwise, the dispersion parameter is no longer as large as before $(\phi=18.43)$ and for this reason it is worth trying to work with this data.  
Due to the fact that over-dispersion is still present, we should use a Quasi-Poisson model; this model would relax the assumption of $\phi=1$. In this new model, the additional over-dispersion parameter $\phi=18.43$ might capture a large amount of the variation of the response variable. Note that since the underlying data is the same, the residuals deviance is the same.  
Before proceeding with a quasi-poisson model otherwise, I prefer to add complexity at the Poisson Regression model and look at the change of the over-dispersion parameter in these more complex models.

**Model 3.**
As said in the previous exploratory data analysis, it is worth trying to transform the explanatory variables $\textit{Distance}$ and $\textit{Population}$ with a logarithmic transformation. This can avoid problems coming from the skewednees these variables.
It is good to notice that the over-dispersion parameter $\phi$ is decreasing too (now it is equal to $3.37$). 
Let's see if also adding interactions between the variables make $\phi$ even lower.

**Model 4.**
Now I am adding at the Poisson regression model in $\textit{Model 3}$ the interactions between the whole variables. So I am keeping the logarithm in the variable and I am adding the interactions.  
In this last model, over-dispersion is not so evident $(\phi=1.46)$ but still it is better to avoid any problem with the dispersion by taking it in some way into account.  Given that the model it is already very complex, I decide to not add further transformations; in this way we have a model that is yes complex, but it is still interpretable in its variable. From now on I will focus on the definitions of the models that that into account the over dispersion and in the selection of the variables to keep in such a model.

**Model 4.1**
The usual way to keep into account the over-dispersion of the model is to fit a quasi-poisson regression model as already said at the start of this section.
```{r}
data <- read_excel("data/data2.xlsx")
data$Direction <- as.factor(data$Direction)
X <- data[-c(1,3)]
y <- data[3]
data$Distance = log(data$Distance)
colnames(data)[colnames(data) == "Distance"] <- "LogDistance"
data$Population = log(data$Population)
colnames(data)[colnames(data) == "Population"] <- "LogPopulation"

poisson.model4<-glm(Apprentices ~ (LogDistance + LogPopulation + Degree_Urb
                    + Direction)^2 , data, family = quasipoisson())
knitr::kable(summary(poisson.model4)$coefficients, digits = 2)
```
As already said before, in this table the p-values are not erroneous and we can notice that there are five variables that are significant at level $\alpha=0.05$. 


